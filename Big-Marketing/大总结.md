# 抽奖接口draw
1. 创建抽奖订单 (`createOrder`): 
- 为用户创建一张抽奖单。检查活动状态和是或否再活动日期内
- 查询是否有状态为created（创建了但未被使用）的订单
- 检查用户是否有足够的抽奖次数（日账户或月账户若不存在则创建），并返回账户构建对象（聚合对象）
- 将订单实体填充进账户构建对象中
- 根据user_id进行分库分表路由，在一个事务中更新总/日/月账户，写入参与活动订单，更新失败则回滚

 ---
    
2. 执行抽奖策略 (`performRaffle`)**: 拿到抽奖单后，调用`raffleStrategy`来执行核心的抽奖算法，决定用户中了什么奖。
- 责任链：负责抽奖**前**的资格审查和初步筛选。它像一条流水线，请求（抽奖用户）在上面依次经过各个检查点，看是否满足特殊规则（如黑名单、权重）
- -   **进入 `BlackListLogicChain`**:
    
    -   **判断**：用户 `userId` 是否在黑名单配置中？
        
    -   **是**：返回安慰奖 `awardId`，流程结束。
        
    -   **否**：调用 `next().logic(...)`，将请求传递给 `RuleWeightLogicChain`。
        
- -   **进入 `RuleWeightLogicChain`**:
    
    -   **判断**：用户的总参与次数是否达到某个权重档位？
        
    -   **是**：从该档位的奖品池抽奖，返回 `awardId`，流程结束。
        
    -   **否**：调用 `next().logic(...)`，将请求传递给 `DefaultLogicChain`。
        
- -  **进入 `DefaultLogicChain`**:
    
    -   **执行**：不再有判断，直接执行全局概率抽奖，返回一个 `awardId`。流程结束。

整体蓝图 - `RuleTreeVO`，决策点 - `RuleTreeNodeVO`，分支路径 - `RuleTreeNodeLineVO`

- 规则树：负责抽奖**后**的结果校验和最终裁定。当责任链给出一个初步的奖品后，规则树会对这个“中奖结果”进行一系列校验（如库存、解锁条件），并给出最终的
- **示例流程**: 假设一个奖品的规则树是 `[次数解锁] -> [库存扣减] -> [兜底奖]`

1.  **进入 `RuleLockLogicTreeNode`**:
    
    -   **决策**：用户抽奖次数是否 > 10次？
        
    -   **是 (`ALLOW`)**：引擎根据 `RuleTreeNodeLine` 的配置，找到 `ALLOW` 分支对应的下一个节点是 `RuleStockLogicTreeNode`。
        
    -   **否 (`TAKE_OVER`)**：引擎找到 `TAKE_OVER` 分支对应的下一个节点是 `RuleLuckAwardLogicTreeNode`（或者直接返回一个“未解锁”的结果）。
        
2.  **进入 `RuleStockLogicTreeNode`**:
    
    -   **决策**：奖品库存 > 0 吗？
        
    -   **是 (`TAKE_OVER`)**：扣减库存成功，引擎“接管”并返回**当前奖品ID**作为最终结果，流程结束。
        
    -   **否 (`ALLOW`)**：库存不足，引擎“放行”，根据 `ALLOW` 分支找到下一个节点是 `RuleLuckAwardLogicTreeNode`。
        
3.  **进入 `RuleLuckAwardLogicTreeNode`**:
    
    -   **决策**：这是一个终点，直接“接管”并返回一个配置好的**兜底奖品ID**。流程结束。
    
-   保存中奖记录 (`saveUserAwardRecord`): 抽奖成功后，调用`awardService`将中奖记录保存到数据库。

 ### 工厂模式的作用
 -   **解耦**：使用者 `DefaultRaffleStrategy` 完全不需要关心责任链内部有多少个节点、它们的顺序是什么。它只需要向工厂请求一个针对特定策略ID的责任链即可。
    
-   **封装创建逻辑**：将复杂的、依赖于配置的链条组装逻辑封装在了工厂内部。如果未来新增了一个“VIP规则”，`DefaultRaffleStrategy`的代码完全不用动，只需要实现新的规则类，并在数据库中配置进去，工厂就能自动把它组装到链条里。

### 具体抽奖算法
1.  **离散化单位（步长）**
    
    -   计算 **最小中奖概率** `minAwardRate` 和 **总概率** `totalAwardRate`。
        
    -   令 `rateRange = ceil(totalAwardRate / minAwardRate)`。  
        含义：用“最小概率”当作一个最小刻度，把所有概率都映射成整数个“格子”。
        
2.  **构建概率查找表（权重展开）**  
    为每个奖品计算 `count = ceil(rateRange * awardRate)`，向列表中**重复插入 `awardId`** `count` 次。  
    这样列表里每个奖品的占位数≈它的概率权重。
    
3.  **打乱顺序**  
    `Collections.shuffle` 随机打乱列表，避免同一奖品的格子扎堆。
    
4.  **转索引映射**  
    把列表转换为 `Map<Integer, Integer>`（`index -> awardId`）。后续会用“随机索引”直查出奖品。

# 消息队列
### **1. 场景一：异步发放奖品**
    
-   **解决方案**:
    
    1.  **核心流程（同步）**: `AwardRepository` 在用户中奖后，只做最核心的两件事：将中奖记录写入数据库（`user_award_record`表），并将一条“待发送的发奖消息”写入`task`表。这两个操作被包裹在一个本地事务中，保证了“中奖”和“发奖消息”的原子性。
        
    2.  **消息发送（事务外）**: 在本地事务成功提交后，立即尝试将发奖消息（`SendAwardMessageEvent`）发布到 RabbitMQ 的 `send_award` 主题。
        
    3.  **非核心流程（异步）**: `SendAwardCustomer` 监听器作为独立的消费者，订阅 `send_award` 主题。当收到消息后，它会解析消息内容，并调用 `IAwardService` 的 `distributeAward` 方法来执行具体的发奖逻辑。
        
-   **效果**: 抽奖接口的响应时间大大缩短，因为它不再等待发奖完成。发奖服务的成功与否不影响用户抽奖的体验，系统整体的吞吐量和可用性得到了提升。
    

### **2. 场景二：异步处理行为返利**

这个场景与异步发奖类似，用于处理用户完成某个行为（如签到）后的奖励发放。

-   **问题**: 用户签到后，系统需要根据预设的返利规则给予奖励，奖励可能是增加抽奖次数（SKU），也可能是增加积分。这些后台操作同样不需要用户实时等待。
    
-   **解决方案**:
    
    1.  **生产者**: `BehaviorRebateService` 在创建返利订单时，同样会将返利任务信息封装成 `SendRebateMessageEvent` 消息，并写入`task`表，然后发布到 `send_rebate` 主题。
        
    2.  **消费者**: `RebateMessageCustomer` 监听 `send_rebate` 主题。收到消息后，它会判断返利类型（`rebateType`）：
        
        -   如果是 `sku`，则调用 `raffleActivityAccountQuotaService` 为用户增加抽奖次数。
            
        -   如果是 `integral`，则调用 `creditAdjustService` 为用户增加积分。
            
-   **效果**: 用户签到接口可以瞬间响应，后台的记账和发奖流程异步执行，实现了流程解耦。
    

### **3. 场景三：领域间通信与最终一致性**

这是更高级的用法，利用消息队列解耦不同领域（Domain）之间的依赖，并通过事件驱动的方式实现最终一致性。

-   **问题**: “积分兑换商品”这个业务，横跨了“积分域（Credit Domain）”和“活动域（Activity Domain）”。流程是：1. 扣减用户积分；2. 为用户增加抽奖次数。如果采用同步调用的方式，两个领域将紧密耦合，且需要处理复杂的分布式事务。
    
-   **解决方案**: **事件驱动架构**
    
    1.  **积分域**: 当用户兑换时，`CreditRepository` 负责**扣减积分**，并在同一个本地事务中，将一个“积分调整成功”的消息（`CreditAdjustSuccessMessageEvent`）写入 `task` 表。
        
    2.  **发布事件**: 事务成功后，积分域发布这个事件到 `credit_adjust_success` 主题。
        
    3.  **活动域**: `CreditAdjustSuccessCustomer` 监听这个主题。收到消息后，它才知道积分已经成功扣除，然后**安全地**调用 `raffleActivityAccountQuotaService.updateOrder` 方法，为用户的活动账户增加相应的抽奖次数。
        
-   **效果**: 积分域和活动域之间没有直接的代码依赖，它们通过消息进行通信。这大大降低了系统的耦合度，使得两个领域可以独立开发和部署。虽然在消息传递的短暂时间内数据可能不一致（积分扣了但次数还没加），但系统最终会达到一致的状态。
    

### **4. 场景四：库存扣减的最终一致性**

-   **问题**: 活动SKU的库存扣减是一个高并发场景。如果每次都直接操作数据库，会给数据库带来巨大压力。
    
-   **解决方案**:
    
    1.  **Redis预扣减**: 绝大部分的库存扣减操作都在高性能的Redis中完成。
        
    2.  **发消息通知**: 当Redis中的库存被扣减为0时，`ActivityRepository`会发布一个`ActivitySkuStockZeroMessageEvent`消息到`activity_sku_stock_zero`主题。
        
    3.  **异步更新DB**: `ActivitySkuStockZeroCustomer`监听到此消息后，会去更新数据库中的库存状态，将其也清零，从而保证了数据库和缓存的最终一致性。
        
-   **效果**: 将库存压力从数据库转移到了Redis，大大提高了系统的并发能力。
    

### **高可用保障：基于`task`表的可靠消息投递**

以上所有场景都依赖于消息的可靠投递。如果生产者发送消息失败（比如RabbitMQ宕机），怎么办？项目采用了一种非常经典的**“事务消息最终一致性”**方案，也叫**“Transactional Outbox”模式**。

1.  **写入`task`表**: 在执行业务操作（如创建中奖记录、创建返利订单）的**同一个本地事务**中，将要发送的MQ消息的完整内容、目标主题等信息，以`state = 'create'`的状态存入`task`表中。
    
2.  **尝试发送**: 事务提交后，程序尝试将消息发送到RabbitMQ。
    
3.  **更新状态**: 如果发送成功，就将`task`表中对应的记录状态更新为`'completed'`。如果失败，则更新为`'fail'`。
    
4.  **定时任务补偿**: 有一个名为`SendMessageTaskJob`的定时任务，它会**每隔5秒**扫描所有分库分表的`task`表，找出所有状态为`'fail'`或长时间处于`'create'`状态的消息。
    
5.  **重新投递**: 定时任务会把这些“失败”的消息重新投递到RabbitMQ，直到成功为止。
    

**效果**: 这种机制确保了即使在消息中间件短暂不可用或网络抖动的情况下，业务消息也**绝对不会丢失**，保证了业务的最终一致性。

综上所述，`Big-Market`项目对消息队列的应用是系统性的、多层次的，它不仅仅是简单地用MQ来异步执行任务，更是将其作为分布式系统架构中解耦、削峰、保证最终一致性的核心组件，并配以强大的可靠性投递机制，是教科书级别的工程实践。

# 接口限流实现细节
1. 定义限流注解@RateLimiterAccessInterceptor
-   `key()`: 用于指定限流的依据。比如，在抽奖接口中，我们希望对每个用户进行限流，那么就可以将`key`设置为`"userId"`。AOP会从请求参数中获取`userId`的值，并为每个`userId`创建一个独立的限流器。
    
-   `permitsPerSecond()`: 这是核心的限流参数，表示**每秒允许的请求次数**。例如，设置为`1.0d`就表示每秒只允许1次请求。
    
-   `blacklistCount()`: 黑名单阈值。如果一个用户在被限流后仍然持续请求，当被拦截的次数超过这个阈值时，该用户就会被加入黑名单（默认24小时），后续所有请求都会被直接拒绝。
    
-   `fallbackMethod()`: 降级方法。当请求被限流时，程序会调用这个指定的方法来返回一个友好的提示，而不是直接报错。

2. 实现AOP切面逻辑`RateLimiterAOP`
-   **动态开关**：`rateLimiterSwitch`字段通过`@DCCValue`注解注入，它的值来自Zookeeper。在AOP逻辑的开始，会先判断这个开关的状态。
    
-   **Guava Cache**: AOP切面中使用了两个Guava的Cache。
    
    -   `loginRecord`: 用来存储每个用户（由`keyAttr`标识）对应的`RateLimiter`实例。设置了1分钟的过期时间，意味着如果一个用户1分钟内没有请求，他的限流器就会被回收，下次请求时会重新创建。
        
    -   `blacklist`: 用来存储被加入黑名单的用户及其被拦截的次数。黑名单有效期为24小时。
        
-   **获取限流器**: 当一个请求进来时，会根据`keyAttr`（例如，`userId`的值 "xiaofuge"）去`loginRecord`缓存中查找对应的`RateLimiter`。如果找不到，就根据注解上配置的`permitsPerSecond`创建一个新的，并存入缓存。
    
-   **`tryAcquire()`**: 这是Guava `RateLimiter`的核心方法。它会尝试获取一个令牌，如果成功（在设定的速率范围内），则返回`true`；如果失败（请求过于频繁），则立即返回`false`。
    
-   **拦截与降级**: 如果`tryAcquire()`返回`false`，说明请求被限流了。此时，程序会更新黑名单计数，并调用注解中指定的`fallbackMethod`方法，将该方法的返回值作为接口的响应返回给用户。
    
-   **放行**: 如果`tryAcquire()`返回`true`，说明请求在允许的频率内，程序会调用`jp.proceed()`来执行原始的接口方法逻辑。

# 接口降级与熔断
### 降级
1. 定义了一个自定义注解`@DCCValue`，用于将Zookeeper中的配置项动态注入到Spring Bean的字段中。
2. 实现Bean后置处理器 (`DCCValueBeanFactory`)
- `DCCValueBeanFactory`会在Spring Bean初始化之后，检查Bean中是否有被`@DCCValue`注解的字段。
- 这个类做了两件核心事情： **初始化注入**: 在Bean初始化时，它会读取`@DCCValue`注解，从Zookeeper中拉取对应节点的数据，并通过反射将值赋给字段。**动态监听**: 它会向Zookeeper注册一个监听器。当Zookeeper中对应的节点数据发生变化时（例如，运维人员通过接口将`degradeSwitch`的值从`"close"`改成`"open"`），监听器会捕捉到这个变化，并实时地更新`RaffleActivityController`中`degradeSwitch`字段的值。

### 熔断
1. 添加Hystrix注解 (`@HystrixCommand`)：设置Hystrixj超时时间为150毫秒，若超过150毫秒自动失败，fallbackMethod：当方法执行超时/方法内部抛出任何未被捕获的异常/在达到一定的失败率后熔断器打开（Open），后续的所有请求都会直接调用降级方法

# Canal与ElasticSearch
“**通过Canal同步binlog至ElasticSearch**” 正是解决这类问题的业界主流方案，它构建了一个**准实时**的数据同步管道，实现了业务数据库（OLTP）和搜索分析引擎（OLAP）的解耦。

1.  **MySQL Binlog**:
    
    -   `binlog`是MySQL Server层记录所有数据更改操作（INSERT, UPDATE, DELETE）的日志文件。
        
    -   MySQL的主从复制（Replication）就是基于这个`binlog`实现的：Master节点将数据变更写入`binlog`，Slave节点获取`binlog`并重放（Replay）这些操作，从而保持与Master的数据一致。
        
2.  **Canal的巧妙之处**:
    
    -   Canal（中文意为“水道”）的核心原理就是**把自己伪装成一个MySQL的Slave节点**。
        
    -   它会像一个真正的Slave一样，向MySQL Master节点发送`dump`协议请求。
        
    -   MySQL Master收到请求后，会持续地将最新的`binlog`事件流推送给Canal。
        
    -   Canal接收到这些二进制的`binlog`事件后，**并不会像真正的Slave那样去执行SQL**，而是对其进行**解析**。它能将`binlog`中复杂、底层的二进制格式，翻译成我们应用程序能够轻松理解的、结构化的数据变更事件（比如：哪个库的哪个表，发生了一次INSERT操作，插入前的数据是什么，插入后的数据是什么）。
        
    -   最后，Canal Server将这些解析好的、结构化的数据变更事件，提供给客户端（我们的消费者服务）进行订阅和消费。
        

**总结一下**：Canal就像一个翻译官和快递员，它不懂数据库的业务，但它能精准地“听懂”数据库的每一次心跳（`binlog`），并把这些心跳翻译成标准普通话，实时地通知给所有订阅了这些消息的系统。

# DECR+SETNX的无锁化兜底设计
当然，你提出的这两点都是分布式系统中非常核心且经典的设计问题。`Big-Market`项目对这两点都给出了非常巧妙的实现。下面我们结合代码，详细剖析这两种“抢占”设计的实现原理。

----------

### **1. 库存抢占：`DECR` + `SETNX` 的无锁化兜底设计**

你对这个设计的理解非常到位：“接近于无锁化的库存编号自增后加锁，做兜底设计，用户的抢占就是 `DECR` 后的结果加锁，降低竞争”。

这套机制的核心目的，是在保证绝对不超卖的前提下，最大限度地降低并发用户在库存上的竞争，提高性能。

#### **代码实现**

这个逻辑在 `ActivityRepository` 和 `StrategyRepository` 中都有体现，我们以活动库存扣减为例。

**文件**: `paulenoch/big-market/Paulenoch-Big-Market-e9e82ccb5179e30cb21d9291c537af8708eaf4d1/Big-Market-infrastructure/src/main/java/com/itheima/infrastructure/persistent/repository/ActivityRepository.java`

Java

```
@Override
public boolean subtractionActivitySkuStock(Long sku, String cacheKey, Date endDateTime) {
    // 步骤 1: “拿号” - 原子递减
    // cacheKey 类似于 "activity_sku_stock_count_key_9011"
    long surplus = redisService.decr(cacheKey);

    // ... 省略库存为0或负数的处理 ...

    // 步骤 2: 生成唯一的“消耗凭证ID”
    // surplus 是 decr 命令返回的、递减后的值。这个值在本次活动库存消耗中是唯一的。
    // lockKey 就变成了类似 "activity_sku_stock_count_key_9011_99" 这样的唯一键。
    String lockKey = cacheKey + Constants.UNDERLINE + surplus;

    // 步骤 3：“锁号” - 为凭证ID加锁，作为兜底
    long expireMillis = endDateTime.getTime() - System.currentTimeMillis() + TimeUnit.DAYS.toMillis(1);
    Boolean lock = redisService.setNx(lockKey, expireMillis, TimeUnit.MILLISECONDS);

    if (!lock) {
        log.info("活动sku库存加锁失败 {}", lockKey);
    }
    
    // 最终返回加锁是否成功
    return lock;
}
```

#### **实现讲解**

1.  **为什么是“接近于无锁化”？**
    
    -   传统的做法是，在扣减库存前，先对`cacheKey`（总库存键）加一个分布式锁，操作完后再释放。在高并发下，所有线程都会去竞争这**同一个锁**，性能会急剧下降。
        
    -   这里的做法是，直接利用 Redis `DECR` 命令的**原子性**。`DECR` 本身就是一种无锁操作，Redis 内部会保证其顺序执行。这一步完成了库存的扣减，速度极快，几乎没有锁竞争。
        
2.  **`DECR` 后的结果有什么用？**
    
    -   `DECR` 不仅减了库存，还返回了减完后的值 `surplus`。假设库存从100开始，连续的请求会依次得到 `99`, `98`, `97`... 这个返回值序列是**唯一且递减**的。
        
    -   这个返回值就像是给每一次成功的库存扣减分配了一个**唯一的编号**。
        
3.  **`SETNX` 兜底设计的精髓**
    
    -   **生成凭证**：代码将 `cacheKey` 和这个唯一的编号 `surplus` 拼接起来，形成了一个类似 `"key_99"` 这样的 `lockKey`。这个 `lockKey` 就代表了“第99号库存单元的消耗凭证”。
        
    -   **锁定凭证**：`SETNX` (SET if Not eXists) 尝试为这个凭证加锁。因为每个凭证ID都是唯一的，所以正常情况下，每个线程都是在对自己**独有的Key**进行`SETNX`操作，**几乎不存在锁竞争**。
        
    -   **为何是兜底？**
        
        -   **幂等性**：如果一个请求因为网络原因重试，它会再次执行相同的 `DECR` 后的逻辑。但由于它第一次已经成功地对（比如）`"key_99"` 进行了 `SETNX`，第二次再对同一个 key 执行 `SETNX` 时就会失败，从而防止了同一次消耗被重复计算。
        - **为补偿和恢复提供依据**：`DECR`只是一个计数器，它本身不包含历史记录。而`SETNX`创建的这些`lockKey`（`..._99`, `..._98`, `..._97`）就像是库存消耗的**交易流水号**。如果系统在后续流程中失败，需要回滚库存，我们可以安全地通过删除对应的`lockKey`并对总库存`INCR`（原子加一）来实现。`lockKey`的存在保证了我们补偿的是一次确实存在的消耗。
            
        -   **防御极端情况**：即使 `DECR` 出现问题（几乎不可能），`SETNX` 也能作为最后一道防线，确保只有一个线程能成功锁定某个库存编号。

# 分布式任务抢占
防止同一个任务被多个实例重复执行
-   **状态更新作为“抢占”**：
    
    -   假设两个服务实例（Instance A 和 Instance B）同时从数据库捞取到了同一个 `task`（因为查询有时间差）。
        
    -   两者都尝试发送MQ消息。
        
    -   之后，两者都会去调用 `updateTaskSendMessageCompleted` 来更新任务状态。
        
    -   这个更新操作并不是简单的 `UPDATE ... WHERE id = ?`，而是带有**状态前置条件**的，例如 `... WHERE id = ? AND state = 'create'`。
        
    -   **Instance A** 先执行 `UPDATE`，成功将 `state` 从 `create` 更新为 `completed`，数据库返回“影响行数为1”。
        
    -   **Instance B** 稍后执行 `UPDATE`，由于此时 `state` 已经是 `completed`，不满足 `WHERE` 子句中的 `state = 'create'` 条件，所以这条 `UPDATE` 语句的**影响行数为0**。
        
-   **结果判断**：在业务代码中，会判断`UPDATE`语句的返回值（影响行数）。
    
    -   返回1，代表抢占成功，任务由我处理。
        
    -   返回0，代表在我处理之前，任务已经被其他实例抢占并处理了，我不需要再做任何事。

# Dubbo提供RPC接口，Nacos作为注册中心，Zookeeper动态配置
-   **Dubbo (RPC框架)**: 负责服务之间的**远程过程调用 (RPC)**。它让调用一个远程服务就像调用本地方法一样简单高效。
    
-   **Nacos (注册中心)**: 负责**服务发现**和**服务注册**。所有 Dubbo 服务启动后，会把自己“注册”到 Nacos 上，消费者则从 Nacos 上“发现”并获取服务提供者的地址列表。
    
-   **ZooKeeper (动态配置中心)**: 负责存储和下发**动态配置**。它让运维人员可以在不重启应用的情况下，实时地修改程序行为（如此前讨论的降级开关）。
    

下面我们来逐一分析它们的实现细节。

----------

### **1. Dubbo RPC 接口：实现服务远程调用**

Dubbo 的核心作用是解耦服务。在这个项目中，虽然目前所有模块都在一个单体应用 (`Big-Market-app`) 中启动，但这种架构设计已经为未来将 `trigger` 层（或其他层）拆分成独立的微服务做好了充分准备。

#### **第一步：API 定义 (`Big-Market-api`)**

所有可以被远程调用的服务接口都定义在 `Big-Market-api` 模块中。这是一个独立的模块，不包含任何实现。它就像一份“合同”，服务提供者和消费者都依赖这份合同进行开发。

**文件**: `paulenoch/big-market/Paulenoch-Big-Market-e9e82ccb5179e30cb21d9291c537af8708eaf4d1/Big-Market-api/src/main/java/com/itheima/trigger/api/IRaffleActivityService.java`

Java

```
// 这是“合同”
public interface IRaffleActivityService {
    Response<ActivityDrawResponseDTO> draw(ActivityDrawRequestDTO request);
    // ... 其他接口定义
}
```

#### **第二步：服务提供者 (`Big-Market-trigger`)**

`trigger` 模块实现了 `api` 模块中定义的接口，并使用 Dubbo 的注解将其暴露为远程服务。

**文件**: `paulenoch/big-market/Paulenoch-Big-Market-e9e82ccb5179e30cb21d9291c537af8708eaf4d1/Big-Market-trigger/src/main/java/com/itheima/trigger/http/RaffleActivityController.java`

Java

```
// ...
@DubboService(version = "1.0") // 使用 @DubboService 注解将这个实现类暴露为 Dubbo 服务
public class RaffleActivityController implements IRaffleActivityService {

    @Override
    public Response<ActivityDrawResponseDTO> draw(@RequestBody ActivityDrawRequestDTO request) {
        // 具体的抽奖实现逻辑
        // ...
    }
    
    // ... 其他接口的实现
}
```

-   **`@DubboService`**: 这个注解是关键。当 Spring 容器初始化这个 Bean 时，Dubbo 框架会识别到这个注解，并通过底层的网络通信框架（如 Netty）为这个服务开启一个端口，准备接收来自其他服务的 RPC 请求。
    

#### **第三步：配置 (`application-dev.yml`)**

为了让 Dubbo 启动并找到注册中心，需要在配置文件中进行配置。

**文件**: `paulenoch/big-market/Paulenoch-Big-Market-e9e82ccb5179e30cb21d9291c537af8708eaf4d1/Big-Market-app/src/main/resources/application-dev.yml`

YAML

```
dubbo:
  application:
    name: big-market # 应用名称
    version: 1.0
  registry:
    id: nacos-registry
    address: nacos://117.72.163.136:8848 # 指定注册中心的地址
  protocol:
    name: dubbo # 使用 dubbo 协议
    port: -1 # 端口号，-1 表示随机分配
  scan:
    base-packages: com.itheima.trigger.api # 扫描 @DubboService 注解所在的包
```

----------

### **2. Nacos 注册中心：服务的“电话簿”**

当服务提供者（`Big-Market-app`）启动时，它怎么告诉别人“我的抽奖服务在 IP 地址 A 的端口 B 上”呢？当服务消费者想调用抽奖服务时，它又怎么知道去哪里找呢？这就是 Nacos 作为注册中心解决的问题。

**工作流程**:

1.  **服务注册**: `Big-Market-app` 启动时，Dubbo 框架会读取 `dubbo.registry.address` 配置，知道要去连接 Nacos。然后，它会将自己暴露的所有服务（如 `IRaffleActivityService`），连同自己的 IP 地址和端口号，一起打包发送给 Nacos 服务器。Nacos 就会记录下：“`IRaffleActivityService` 的提供者地址是 A:B”。
    
2.  **服务发现**: 假设未来有一个新的 `Order-Service` (订单服务) 需要调用抽奖服务。`Order-Service` 只需要在代码中注入 `IRaffleActivityService` 接口，并使用 Dubbo 的 `@DubboReference` 注解。启动时，`Order-Service` 也会连接 Nacos，并询问：“谁能提供 `IRaffleActivityService` 服务？”
    
3.  **地址拉取与调用**: Nacos 会将它记录的地址列表 `[A:B, C:D, ...]` 返回给 `Order-Service`。`Order-Service` 的 Dubbo 客户端会缓存这个地址列表，并通过负载均衡算法（如随机、轮询）选择一个地址，发起 RPC 调用。
    
4.  **健康检查**: Nacos 还会与服务提供者保持心跳。如果 `Big-Market-app` 宕机了，Nacos 会及时发现并将其从服务列表中剔除，避免消费者调用到一个已经不可用的服务。
    

----------

### **3. ZooKeeper 动态配置中心：行为的“遥控器”**

我们之前已经详细讨论过 ZooKeeper 在手动降级中的应用，这里我们再次回顾并总结一下它的角色。

**核心区别**:

-   **Nacos** 在这个项目中主要用于**服务元数据**的管理（哪个服务在哪里）。
    
-   **ZooKeeper** 则用于**业务应用配置**的管理（某个功能是否开启）。
    

**工作流程**:

1.  **配置存储**: `degradeSwitch` 这样的配置项，其值（`"open"` 或 `"close"`）存储在 ZooKeeper 的一个特定节点（ZNode）上，路径为 `/big-market-dcc/config/degradeSwitch`。
    
2.  **启动时拉取**: 应用启动时，`DCCValueBeanFactory` 会连接到 ZooKeeper（根据 `zookeeper.sdk.config.connect-string` 配置），读取这个节点的值，并注入到 `RaffleActivityController` 的 `degradeSwitch` 字段中。
    
3.  **动态监听与更新**: `DCCValueBeanFactory` 会在 ZNode 上注册一个 **Watcher (监听器)**。
    
4.  **远程控制**: 当运维人员通过 `DCCController` 的接口修改了 ZooKeeper 上 `degradeSwitch` 节点的值时，ZooKeeper 会立即通知所有监听了这个节点的客户端（也就是所有 `Big-Market-app` 的实例）。
    
5.  **实时生效**: `DCCValueBeanFactory` 的监听器被触发，它会获取新的值并**实时更新**所有应用实例中对应的 `degradeSwitch` 字段。这样，降级开关的变更就实现了**动态生效**，无需重启服务。

# 如果同时有100个订单下单，100个订单都被延迟3秒，那岂不是还是100个请求同时打到数据库上
整个流程是这样的：

-   **削峰**：100个并发请求首先在Redis中快速完成库存扣减，并将数据库更新任务放入延迟队列。这个过程非常快，成功地应对了瞬时流量高峰。
    
-   **填谷**：虽然这100个任务在3秒后会同时变为可消费状态，但后台的定时任务是**每5秒才从队列里取一个来处理**。这样就把集中的并发请求分散成了平稳、低频的数据库操作。
因此，数据库并不会在3秒后同时收到100个请求，而是会以每5秒1次的频率，平稳地、逐个地处理这些库存更新请求。这种“延迟队列 + 定时轮询”的模式是处理高并发写入场景的一种经典有效的解决方案。
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTEzNDQxNzEwOCwxMDI5Mjg1NDMsLTE0Mj
Q1MjAxNzEsLTIwNzMyMDU4NDYsMTY1NTA3NjYyMywyMDI1Njg5
NDE2LC03OTU1NDAwMCwtMzQ3ODUwODMsMTk2NjcwNDQyOCw1MD
UzMDk4NCwtMTI5NTUyNTE0NywxNTY5MTEzNzYxXX0=
-->