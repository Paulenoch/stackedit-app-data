# 抽奖接口draw
1. 创建抽奖订单 (`createOrder`): 
- 为用户创建一张抽奖单。检查活动状态和是或否再活动日期内
- 查询是否有状态为created（创建了但未被使用）的订单
- 检查用户是否有足够的抽奖次数（日账户或月账户若不存在则创建），并返回账户构建对象（聚合对象）
- 将订单实体填充进账户构建对象中
- 根据user_id进行分库分表路由，在一个事务中更新总/日/月账户，写入参与活动订单，更新失败则回滚

 ---
    
2. 执行抽奖策略 (`performRaffle`)**: 拿到抽奖单后，调用`raffleStrategy`来执行核心的抽奖算法，决定用户中了什么奖。
- 责任链：负责抽奖**前**的资格审查和初步筛选。它像一条流水线，请求（抽奖用户）在上面依次经过各个检查点，看是否满足特殊规则（如黑名单、权重）
- -   **进入 `BlackListLogicChain`**:
    
    -   **判断**：用户 `userId` 是否在黑名单配置中？
        
    -   **是**：返回安慰奖 `awardId`，流程结束。
        
    -   **否**：调用 `next().logic(...)`，将请求传递给 `RuleWeightLogicChain`。
        
- -   **进入 `RuleWeightLogicChain`**:
    
    -   **判断**：用户的总参与次数是否达到某个权重档位？
        
    -   **是**：从该档位的奖品池抽奖，返回 `awardId`，流程结束。
        
    -   **否**：调用 `next().logic(...)`，将请求传递给 `DefaultLogicChain`。
        
- -  **进入 `DefaultLogicChain`**:
    
    -   **执行**：不再有判断，直接执行全局概率抽奖，返回一个 `awardId`。流程结束。

整体蓝图 - `RuleTreeVO`，决策点 - `RuleTreeNodeVO`，分支路径 - `RuleTreeNodeLineVO`

- 规则树：负责抽奖**后**的结果校验和最终裁定。当责任链给出一个初步的奖品后，规则树会对这个“中奖结果”进行一系列校验（如库存、解锁条件），并给出最终的
- **示例流程**: 假设一个奖品的规则树是 `[次数解锁] -> [库存扣减] -> [兜底奖]`

1.  **进入 `RuleLockLogicTreeNode`**:
    
    -   **决策**：用户抽奖次数是否 > 10次？
        
    -   **是 (`ALLOW`)**：引擎根据 `RuleTreeNodeLine` 的配置，找到 `ALLOW` 分支对应的下一个节点是 `RuleStockLogicTreeNode`。
        
    -   **否 (`TAKE_OVER`)**：引擎找到 `TAKE_OVER` 分支对应的下一个节点是 `RuleLuckAwardLogicTreeNode`（或者直接返回一个“未解锁”的结果）。
        
2.  **进入 `RuleStockLogicTreeNode`**:
    
    -   **决策**：奖品库存 > 0 吗？
        
    -   **是 (`TAKE_OVER`)**：扣减库存成功，引擎“接管”并返回**当前奖品ID**作为最终结果，流程结束。
        
    -   **否 (`ALLOW`)**：库存不足，引擎“放行”，根据 `ALLOW` 分支找到下一个节点是 `RuleLuckAwardLogicTreeNode`。
        
3.  **进入 `RuleLuckAwardLogicTreeNode`**:
    
    -   **决策**：这是一个终点，直接“接管”并返回一个配置好的**兜底奖品ID**。流程结束。
    
-   保存中奖记录 (`saveUserAwardRecord`): 抽奖成功后，调用`awardService`将中奖记录保存到数据库。

 ### 工厂模式的作用
 -   **解耦**：使用者 `DefaultRaffleStrategy` 完全不需要关心责任链内部有多少个节点、它们的顺序是什么。它只需要向工厂请求一个针对特定策略ID的责任链即可。
    
-   **封装创建逻辑**：将复杂的、依赖于配置的链条组装逻辑封装在了工厂内部。如果未来新增了一个“VIP规则”，`DefaultRaffleStrategy`的代码完全不用动，只需要实现新的规则类，并在数据库中配置进去，工厂就能自动把它组装到链条里。

### 具体抽奖算法
1.  **离散化单位（步长）**
    
    -   计算 **最小中奖概率** `minAwardRate` 和 **总概率** `totalAwardRate`。
        
    -   令 `rateRange = ceil(totalAwardRate / minAwardRate)`。  
        含义：用“最小概率”当作一个最小刻度，把所有概率都映射成整数个“格子”。
        
2.  **构建概率查找表（权重展开）**  
    为每个奖品计算 `count = ceil(rateRange * awardRate)`，向列表中**重复插入 `awardId`** `count` 次。  
    这样列表里每个奖品的占位数≈它的概率权重。
    
3.  **打乱顺序**  
    `Collections.shuffle` 随机打乱列表，避免同一奖品的格子扎堆。
    
4.  **转索引映射**  
    把列表转换为 `Map<Integer, Integer>`（`index -> awardId`）。后续会用“随机索引”直查出奖品。

# 消息队列
### **1. 场景一：异步发放奖品**
    
-   **解决方案**:
    
    1.  **核心流程（同步）**: `AwardRepository` 在用户中奖后，只做最核心的两件事：将中奖记录写入数据库（`user_award_record`表），并将一条“待发送的发奖消息”写入`task`表。这两个操作被包裹在一个本地事务中，保证了“中奖”和“发奖消息”的原子性。
        
    2.  **消息发送（事务外）**: 在本地事务成功提交后，立即尝试将发奖消息（`SendAwardMessageEvent`）发布到 RabbitMQ 的 `send_award` 主题。
        
    3.  **非核心流程（异步）**: `SendAwardCustomer` 监听器作为独立的消费者，订阅 `send_award` 主题。当收到消息后，它会解析消息内容，并调用 `IAwardService` 的 `distributeAward` 方法来执行具体的发奖逻辑。
        
-   **效果**: 抽奖接口的响应时间大大缩短，因为它不再等待发奖完成。发奖服务的成功与否不影响用户抽奖的体验，系统整体的吞吐量和可用性得到了提升。
    

### **2. 场景二：异步处理行为返利**

这个场景与异步发奖类似，用于处理用户完成某个行为（如签到）后的奖励发放。

-   **问题**: 用户签到后，系统需要根据预设的返利规则给予奖励，奖励可能是增加抽奖次数（SKU），也可能是增加积分。这些后台操作同样不需要用户实时等待。
    
-   **解决方案**:
    
    1.  **生产者**: `BehaviorRebateService` 在创建返利订单时，同样会将返利任务信息封装成 `SendRebateMessageEvent` 消息，并写入`task`表，然后发布到 `send_rebate` 主题。
        
    2.  **消费者**: `RebateMessageCustomer` 监听 `send_rebate` 主题。收到消息后，它会判断返利类型（`rebateType`）：
        
        -   如果是 `sku`，则调用 `raffleActivityAccountQuotaService` 为用户增加抽奖次数。
            
        -   如果是 `integral`，则调用 `creditAdjustService` 为用户增加积分。
            
-   **效果**: 用户签到接口可以瞬间响应，后台的记账和发奖流程异步执行，实现了流程解耦。
    

### **3. 场景三：领域间通信与最终一致性**

这是更高级的用法，利用消息队列解耦不同领域（Domain）之间的依赖，并通过事件驱动的方式实现最终一致性。

-   **问题**: “积分兑换商品”这个业务，横跨了“积分域（Credit Domain）”和“活动域（Activity Domain）”。流程是：1. 扣减用户积分；2. 为用户增加抽奖次数。如果采用同步调用的方式，两个领域将紧密耦合，且需要处理复杂的分布式事务。
    
-   **解决方案**: **事件驱动架构**
    
    1.  **积分域**: 当用户兑换时，`CreditRepository` 负责**扣减积分**，并在同一个本地事务中，将一个“积分调整成功”的消息（`CreditAdjustSuccessMessageEvent`）写入 `task` 表。
        
    2.  **发布事件**: 事务成功后，积分域发布这个事件到 `credit_adjust_success` 主题。
        
    3.  **活动域**: `CreditAdjustSuccessCustomer` 监听这个主题。收到消息后，它才知道积分已经成功扣除，然后**安全地**调用 `raffleActivityAccountQuotaService.updateOrder` 方法，为用户的活动账户增加相应的抽奖次数。
        
-   **效果**: 积分域和活动域之间没有直接的代码依赖，它们通过消息进行通信。这大大降低了系统的耦合度，使得两个领域可以独立开发和部署。虽然在消息传递的短暂时间内数据可能不一致（积分扣了但次数还没加），但系统最终会达到一致的状态。
    

### **4. 场景四：库存扣减的最终一致性**

-   **问题**: 活动SKU的库存扣减是一个高并发场景。如果每次都直接操作数据库，会给数据库带来巨大压力。
    
-   **解决方案**:
    
    1.  **Redis预扣减**: 绝大部分的库存扣减操作都在高性能的Redis中完成。
        
    2.  **发消息通知**: 当Redis中的库存被扣减为0时，`ActivityRepository`会发布一个`ActivitySkuStockZeroMessageEvent`消息到`activity_sku_stock_zero`主题。
        
    3.  **异步更新DB**: `ActivitySkuStockZeroCustomer`监听到此消息后，会去更新数据库中的库存状态，将其也清零，从而保证了数据库和缓存的最终一致性。
        
-   **效果**: 将库存压力从数据库转移到了Redis，大大提高了系统的并发能力。
    

### **高可用保障：基于`task`表的可靠消息投递**

以上所有场景都依赖于消息的可靠投递。如果生产者发送消息失败（比如RabbitMQ宕机），怎么办？项目采用了一种非常经典的**“事务消息最终一致性”**方案，也叫**“Transactional Outbox”模式**。

1.  **写入`task`表**: 在执行业务操作（如创建中奖记录、创建返利订单）的**同一个本地事务**中，将要发送的MQ消息的完整内容、目标主题等信息，以`state = 'create'`的状态存入`task`表中。
    
2.  **尝试发送**: 事务提交后，程序尝试将消息发送到RabbitMQ。
    
3.  **更新状态**: 如果发送成功，就将`task`表中对应的记录状态更新为`'completed'`。如果失败，则更新为`'fail'`。
    
4.  **定时任务补偿**: 有一个名为`SendMessageTaskJob`的定时任务，它会**每隔5秒**扫描所有分库分表的`task`表，找出所有状态为`'fail'`或长时间处于`'create'`状态的消息。
    
5.  **重新投递**: 定时任务会把这些“失败”的消息重新投递到RabbitMQ，直到成功为止。
    

**效果**: 这种机制确保了即使在消息中间件短暂不可用或网络抖动的情况下，业务消息也**绝对不会丢失**，保证了业务的最终一致性。

综上所述，`Big-Market`项目对消息队列的应用是系统性的、多层次的，它不仅仅是简单地用MQ来异步执行任务，更是将其作为分布式系统架构中解耦、削峰、保证最终一致性的核心组件，并配以强大的可靠性投递机制，是教科书级别的工程实践。

# 接口限流实现细节
1. 定义限流注解@RateLimiterAccessInterceptor
-   `key()`: 用于指定限流的依据。比如，在抽奖接口中，我们希望对每个用户进行限流，那么就可以将`key`设置为`"userId"`。AOP会从请求参数中获取`userId`的值，并为每个`userId`创建一个独立的限流器。
    
-   `permitsPerSecond()`: 这是核心的限流参数，表示**每秒允许的请求次数**。例如，设置为`1.0d`就表示每秒只允许1次请求。
    
-   `blacklistCount()`: 黑名单阈值。如果一个用户在被限流后仍然持续请求，当被拦截的次数超过这个阈值时，该用户就会被加入黑名单（默认24小时），后续所有请求都会被直接拒绝。
    
-   `fallbackMethod()`: 降级方法。当请求被限流时，程序会调用这个指定的方法来返回一个友好的提示，而不是直接报错。

2. 实现AOP切面逻辑`RateLimiterAOP`
-   **动态开关**：`rateLimiterSwitch`字段通过`@DCCValue`注解注入，它的值来自Zookeeper。在AOP逻辑的开始，会先判断这个开关的状态。
    
-   **Guava Cache**: AOP切面中使用了两个Guava的Cache。
    
    -   `loginRecord`: 用来存储每个用户（由`keyAttr`标识）对应的`RateLimiter`实例。设置了1分钟的过期时间，意味着如果一个用户1分钟内没有请求，他的限流器就会被回收，下次请求时会重新创建。
        
    -   `blacklist`: 用来存储被加入黑名单的用户及其被拦截的次数。黑名单有效期为24小时。
        
-   **获取限流器**: 当一个请求进来时，会根据`keyAttr`（例如，`userId`的值 "xiaofuge"）去`loginRecord`缓存中查找对应的`RateLimiter`。如果找不到，就根据注解上配置的`permitsPerSecond`创建一个新的，并存入缓存。
    
-   **`tryAcquire()`**: 这是Guava `RateLimiter`的核心方法。它会尝试获取一个令牌，如果成功（在设定的速率范围内），则返回`true`；如果失败（请求过于频繁），则立即返回`false`。
    
-   **拦截与降级**: 如果`tryAcquire()`返回`false`，说明请求被限流了。此时，程序会更新黑名单计数，并调用注解中指定的`fallbackMethod`方法，将该方法的返回值作为接口的响应返回给用户。
    
-   **放行**: 如果`tryAcquire()`返回`true`，说明请求在允许的频率内，程序会调用`jp.proceed()`来执行原始的接口方法逻辑。

# 接口降级与熔断
### 降级
1. 定义了一个自定义注解`@DCCValue`，用于将Zookeeper中的配置项动态注入到Spring Bean的字段中。
2. 实现Bean后置处理器 (`DCCValueBeanFactory`)
- `DCCValueBeanFactory`会在Spring Bean初始化之后，检查Bean中是否有被`@DCCValue`注解的字段。
- 这个类做了两件核心事情： **初始化注入**: 在Bean初始化时，它会读取`@DCCValue`注解，从Zookeeper中拉取对应节点的数据，并通过反射将值赋给字段。**动态监听**: 它会向Zookeeper注册一个监听器。当Zookeeper中对应的节点数据发生变化时（例如，运维人员通过接口将`degradeSwitch`的值从`"close"`改成`"open"`），监听器会捕捉到这个变化，并实时地更新`RaffleActivityController`中`degradeSwitch`字段的值。

### 熔断
1. 添加Hystrix注解 (`@HystrixCommand`)：设置Hystrixj超时时间为150毫秒，若超过150毫秒自动失败，fallbackMethod：当方法执行超时/方法内部抛出任何未被捕获的异常/在达到一定的失败率后熔断器打开（Open），后续的所有请求都会直接调用降级方法

# Canal与ElasticSearch
“**通过Canal同步binlog至ElasticSearch**” 正是解决这类问题的业界主流方案，它构建了一个**准实时**的数据同步管道，实现了业务数据库（OLTP）和搜索分析引擎（OLAP）的解耦。

1.  **MySQL Binlog**:
    
    -   `binlog`是MySQL Server层记录所有数据更改操作（INSERT, UPDATE, DELETE）的日志文件。
        
    -   MySQL的主从复制（Replication）就是基于这个`binlog`实现的：Master节点将数据变更写入`binlog`，Slave节点获取`binlog`并重放（Replay）这些操作，从而保持与Master的数据一致。
        
2.  **Canal的巧妙之处**:
    
    -   Canal（中文意为“水道”）的核心原理就是**把自己伪装成一个MySQL的Slave节点**。
        
    -   它会像一个真正的Slave一样，向MySQL Master节点发送`dump`协议请求。
        
    -   MySQL Master收到请求后，会持续地将最新的`binlog`事件流推送给Canal。
        
    -   Canal接收到这些二进制的`binlog`事件后，**并不会像真正的Slave那样去执行SQL**，而是对其进行**解析**。它能将`binlog`中复杂、底层的二进制格式，翻译成我们应用程序能够轻松理解的、结构化的数据变更事件（比如：哪个库的哪个表，发生了一次INSERT操作，插入前的数据是什么，插入后的数据是什么）。
        
    -   最后，Canal Server将这些解析好的、结构化的数据变更事件，提供给客户端（我们的消费者服务）进行订阅和消费。
        

**总结一下**：Canal就像一个翻译官和快递员，它不懂数据库的业务，但它能精准地“听懂”数据库的每一次心跳（`binlog`），并把这些心跳翻译成标准普通话，实时地通知给所有订阅了这些消息的系统。

# DECR+SETNX的无锁化兜底设计
当然，你提出的这两点都是分布式系统中非常核心且经典的设计问题。`Big-Market`项目对这两点都给出了非常巧妙的实现。下面我们结合代码，详细剖析这两种“抢占”设计的实现原理。

----------

### **1. 库存抢占：`DECR` + `SETNX` 的无锁化兜底设计**

你对这个设计的理解非常到位：“接近于无锁化的库存编号自增后加锁，做兜底设计，用户的抢占就是 `DECR` 后的结果加锁，降低竞争”。

这套机制的核心目的，是在保证绝对不超卖的前提下，最大限度地降低并发用户在库存上的竞争，提高性能。

#### **代码实现**

这个逻辑在 `ActivityRepository` 和 `StrategyRepository` 中都有体现，我们以活动库存扣减为例。

**文件**: `paulenoch/big-market/Paulenoch-Big-Market-e9e82ccb5179e30cb21d9291c537af8708eaf4d1/Big-Market-infrastructure/src/main/java/com/itheima/infrastructure/persistent/repository/ActivityRepository.java`


```java
@Override
public boolean subtractionActivitySkuStock(Long sku, String cacheKey, Date endDateTime) {
    // 步骤 1: “拿号” - 原子递减
    // cacheKey 类似于 "activity_sku_stock_count_key_9011"
    long surplus = redisService.decr(cacheKey);

    // ... 省略库存为0或负数的处理 ...

    // 步骤 2: 生成唯一的“消耗凭证ID”
    // surplus 是 decr 命令返回的、递减后的值。这个值在本次活动库存消耗中是唯一的。
    // lockKey 就变成了类似 "activity_sku_stock_count_key_9011_99" 这样的唯一键。
    String lockKey = cacheKey + Constants.UNDERLINE + surplus;

    // 步骤 3：“锁号” - 为凭证ID加锁，作为兜底
    long expireMillis = endDateTime.getTime() - System.currentTimeMillis() + TimeUnit.DAYS.toMillis(1);
    Boolean lock = redisService.setNx(lockKey, expireMillis, TimeUnit.MILLISECONDS);

    if (!lock) {
        log.info("活动sku库存加锁失败 {}", lockKey);
    }
    
    // 最终返回加锁是否成功
    return lock;
}
```

#### **实现讲解**

1.  **为什么是“接近于无锁化”？**
    
    -   传统的做法是，在扣减库存前，先对`cacheKey`（总库存键）加一个分布式锁，操作完后再释放。在高并发下，所有线程都会去竞争这**同一个锁**，性能会急剧下降。
        
    -   这里的做法是，直接利用 Redis `DECR` 命令的**原子性**。`DECR` 本身就是一种无锁操作，Redis 内部会保证其顺序执行。这一步完成了库存的扣减，速度极快，几乎没有锁竞争。
        
2.  **`DECR` 后的结果有什么用？**
    
    -   `DECR` 不仅减了库存，还返回了减完后的值 `surplus`。假设库存从100开始，连续的请求会依次得到 `99`, `98`, `97`... 这个返回值序列是**唯一且递减**的。
        
    -   这个返回值就像是给每一次成功的库存扣减分配了一个**唯一的编号**。
        
3.  **`SETNX` 兜底设计的精髓**
    
    -   **生成凭证**：代码将 `cacheKey` (固定前缀+活动id+奖品id）和这个唯一的编号 `surplus` 拼接起来，形成了一个类似 `"key_99"` 这样的 `lockKey`。这个 `lockKey` 就代表了“第99号库存单元的消耗凭证”。
        
    -   **锁定凭证**：`SETNX` (SET if Not eXists) 尝试为这个凭证加锁。因为每个凭证ID都是唯一的，所以正常情况下，每个线程都是在对自己**独有的Key**进行`SETNX`操作，**几乎不存在锁竞争**。
        
    -   **为何是兜底？**
        
        -   **幂等性**：如果一个请求因为网络原因重试，它会再次执行相同的 `DECR` 后的逻辑。但由于它第一次已经成功地对（比如）`"key_99"` 进行了 `SETNX`，第二次再对同一个 key 执行 `SETNX` 时就会失败，从而防止了同一次消耗被重复计算。
        - **为补偿和恢复提供依据**：`DECR`只是一个计数器，它本身不包含历史记录。而`SETNX`创建的这些`lockKey`（`..._99`, `..._98`, `..._97`）就像是库存消耗的**交易流水号**。如果系统在后续流程中失败，需要回滚库存，我们可以安全地通过删除对应的`lockKey`并对总库存`INCR`（原子加一）来实现。`lockKey`的存在保证了我们补偿的是一次确实存在的消耗。
            
        -   **防御极端情况**：即使 `DECR` 出现问题（几乎不可能），`SETNX` 也能作为最后一道防线，确保只有一个线程能成功锁定某个库存编号。

# 分布式任务抢占
防止同一个任务被多个实例重复执行
-   **状态更新作为“抢占”**：
    
    -   假设两个服务实例（Instance A 和 Instance B）同时从数据库捞取到了同一个 `task`（因为查询有时间差）。
        
    -   两者都尝试发送MQ消息。
        
    -   之后，两者都会去调用 `updateTaskSendMessageCompleted` 来更新任务状态。
        
    -   这个更新操作并不是简单的 `UPDATE ... WHERE id = ?`，而是带有**状态前置条件**的，例如 `... WHERE id = ? AND state = 'create'`。
        
    -   **Instance A** 先执行 `UPDATE`，成功将 `state` 从 `create` 更新为 `completed`，数据库返回“影响行数为1”。
        
    -   **Instance B** 稍后执行 `UPDATE`，由于此时 `state` 已经是 `completed`，不满足 `WHERE` 子句中的 `state = 'create'` 条件，所以这条 `UPDATE` 语句的**影响行数为0**。
        
-   **结果判断**：在业务代码中，会判断`UPDATE`语句的返回值（影响行数）。
    
    -   返回1，代表抢占成功，任务由我处理。
        
    -   返回0，代表在我处理之前，任务已经被其他实例抢占并处理了，我不需要再做任何事。

# Dubbo提供RPC接口，Nacos作为注册中心，Zookeeper动态配置
-   **Dubbo (RPC框架)**: 负责服务之间的**远程过程调用 (RPC)**。它让调用一个远程服务就像调用本地方法一样简单高效。
    
-   **Nacos (注册中心)**: 负责**服务发现**和**服务注册**。所有 Dubbo 服务启动后，会把自己“注册”到 Nacos 上，消费者则从 Nacos 上“发现”并获取服务提供者的地址列表。
    
-   **ZooKeeper (动态配置中心)**: 负责存储和下发**动态配置**。它让运维人员可以在不重启应用的情况下，实时地修改程序行为（如此前讨论的降级开关）。
    

下面我们来逐一分析它们的实现细节。

----------

### **1. Dubbo RPC 接口：实现服务远程调用**

Dubbo 的核心作用是解耦服务。在这个项目中，虽然目前所有模块都在一个单体应用 (`Big-Market-app`) 中启动，但这种架构设计已经为未来将 `trigger` 层（或其他层）拆分成独立的微服务做好了充分准备。

#### **第一步：API 定义 (`Big-Market-api`)**

所有可以被远程调用的服务接口都定义在 `Big-Market-api` 模块中。这是一个独立的模块，不包含任何实现。它就像一份“合同”，服务提供者和消费者都依赖这份合同进行开发。

**文件**: `paulenoch/big-market/Paulenoch-Big-Market-e9e82ccb5179e30cb21d9291c537af8708eaf4d1/Big-Market-api/src/main/java/com/itheima/trigger/api/IRaffleActivityService.java`

Java

```
// 这是“合同”
public interface IRaffleActivityService {
    Response<ActivityDrawResponseDTO> draw(ActivityDrawRequestDTO request);
    // ... 其他接口定义
}
```

#### **第二步：服务提供者 (`Big-Market-trigger`)**

`trigger` 模块实现了 `api` 模块中定义的接口，并使用 Dubbo 的注解将其暴露为远程服务。

**文件**: `paulenoch/big-market/Paulenoch-Big-Market-e9e82ccb5179e30cb21d9291c537af8708eaf4d1/Big-Market-trigger/src/main/java/com/itheima/trigger/http/RaffleActivityController.java`

Java

```
// ...
@DubboService(version = "1.0") // 使用 @DubboService 注解将这个实现类暴露为 Dubbo 服务
public class RaffleActivityController implements IRaffleActivityService {

    @Override
    public Response<ActivityDrawResponseDTO> draw(@RequestBody ActivityDrawRequestDTO request) {
        // 具体的抽奖实现逻辑
        // ...
    }
    
    // ... 其他接口的实现
}
```

-   **`@DubboService`**: 这个注解是关键。当 Spring 容器初始化这个 Bean 时，Dubbo 框架会识别到这个注解，并通过底层的网络通信框架（如 Netty）为这个服务开启一个端口，准备接收来自其他服务的 RPC 请求。
    

#### **第三步：配置 (`application-dev.yml`)**

为了让 Dubbo 启动并找到注册中心，需要在配置文件中进行配置。

**文件**: `paulenoch/big-market/Paulenoch-Big-Market-e9e82ccb5179e30cb21d9291c537af8708eaf4d1/Big-Market-app/src/main/resources/application-dev.yml`

YAML

```
dubbo:
  application:
    name: big-market # 应用名称
    version: 1.0
  registry:
    id: nacos-registry
    address: nacos://117.72.163.136:8848 # 指定注册中心的地址
  protocol:
    name: dubbo # 使用 dubbo 协议
    port: -1 # 端口号，-1 表示随机分配
  scan:
    base-packages: com.itheima.trigger.api # 扫描 @DubboService 注解所在的包
```

----------

### **2. Nacos 注册中心：服务的“电话簿”**

当服务提供者（`Big-Market-app`）启动时，它怎么告诉别人“我的抽奖服务在 IP 地址 A 的端口 B 上”呢？当服务消费者想调用抽奖服务时，它又怎么知道去哪里找呢？这就是 Nacos 作为注册中心解决的问题。

**工作流程**:

1.  **服务注册**: `Big-Market-app` 启动时，Dubbo 框架会读取 `dubbo.registry.address` 配置，知道要去连接 Nacos。然后，它会将自己暴露的所有服务（如 `IRaffleActivityService`），连同自己的 IP 地址和端口号，一起打包发送给 Nacos 服务器。Nacos 就会记录下：“`IRaffleActivityService` 的提供者地址是 A:B”。
    
2.  **服务发现**: 假设未来有一个新的 `Order-Service` (订单服务) 需要调用抽奖服务。`Order-Service` 只需要在代码中注入 `IRaffleActivityService` 接口，并使用 Dubbo 的 `@DubboReference` 注解。启动时，`Order-Service` 也会连接 Nacos，并询问：“谁能提供 `IRaffleActivityService` 服务？”
    
3.  **地址拉取与调用**: Nacos 会将它记录的地址列表 `[A:B, C:D, ...]` 返回给 `Order-Service`。`Order-Service` 的 Dubbo 客户端会缓存这个地址列表，并通过负载均衡算法（如随机、轮询）选择一个地址，发起 RPC 调用。
    
4.  **健康检查**: Nacos 还会与服务提供者保持心跳。如果 `Big-Market-app` 宕机了，Nacos 会及时发现并将其从服务列表中剔除，避免消费者调用到一个已经不可用的服务。
    

----------

### **3. ZooKeeper 动态配置中心：行为的“遥控器”**

我们之前已经详细讨论过 ZooKeeper 在手动降级中的应用，这里我们再次回顾并总结一下它的角色。

**核心区别**:

-   **Nacos** 在这个项目中主要用于**服务元数据**的管理（哪个服务在哪里）。
    
-   **ZooKeeper** 则用于**业务应用配置**的管理（某个功能是否开启）。
    

**工作流程**:

1.  **配置存储**: `degradeSwitch` 这样的配置项，其值（`"open"` 或 `"close"`）存储在 ZooKeeper 的一个特定节点（ZNode）上，路径为 `/big-market-dcc/config/degradeSwitch`。
    
2.  **启动时拉取**: 应用启动时，`DCCValueBeanFactory` 会连接到 ZooKeeper（根据 `zookeeper.sdk.config.connect-string` 配置），读取这个节点的值，并注入到 `RaffleActivityController` 的 `degradeSwitch` 字段中。
    
3.  **动态监听与更新**: `DCCValueBeanFactory` 会在 ZNode 上注册一个 **Watcher (监听器)**。
    
4.  **远程控制**: 当运维人员通过 `DCCController` 的接口修改了 ZooKeeper 上 `degradeSwitch` 节点的值时，ZooKeeper 会立即通知所有监听了这个节点的客户端（也就是所有 `Big-Market-app` 的实例）。
    
5.  **实时生效**: `DCCValueBeanFactory` 的监听器被触发，它会获取新的值并**实时更新**所有应用实例中对应的 `degradeSwitch` 字段。这样，降级开关的变更就实现了**动态生效**，无需重启服务。

# 如果同时有100个订单下单，100个订单都被延迟3秒，那岂不是还是100个请求同时打到数据库上
整个流程是这样的：

-   **削峰**：100个并发请求首先在Redis中快速完成库存扣减，并将数据库更新任务放入延迟队列。这个过程非常快，成功地应对了瞬时流量高峰。
    
-   **填谷**：虽然这100个任务在3秒后会同时变为可消费状态，但后台的定时任务是**每5秒才从队列里取一个来处理**。这样就把集中的并发请求分散成了平稳、低频的数据库操作。
因此，数据库并不会在3秒后同时收到100个请求，而是会以每5秒1次的频率，平稳地、逐个地处理这些库存更新请求。这种“延迟队列 + 定时轮询”的模式是处理高并发写入场景的一种经典有效的解决方案。


# 通过canal同步mysql的数据到es中，会不会es端先接收到时间较新的binlog，后接受到旧的binlog导致最终数据不一致
### 1. MySQL 和 Canal 如何保证顺序？ (数据源头)

这是顺序性的基础和保障。

-   **MySQL Binlog 的顺序性**：MySQL 的 `binlog` 是一个严格的、串行的日志文件。事务是按照它们**提交（commit）的顺序**被写入到 binlog 中的。每一个事件在 binlog 中都有一个唯一且递增的位置（position 或者 GTID）。这是数据顺序的“第一事实来源”。
    
-   **Canal 的顺序消费**：Canal 模拟了一个 MySQL 的 slave 节点，它会向 MySQL master 请求指定位置（position/GTID）之后的 binlog。Canal 的内部解析器（Parser）是**单线程**的，它会严格按照 binlog 文件中的事件顺序来逐个解析事件。解析完成后，也是按顺序将数据投递给后续的 Sink 模块（例如投递到 Kafka）。
    

**结论**：在 `MySQL -> Canal Server` 这一阶段，顺序是**绝对有保障的**。
### 2. 哪里可能出现乱序？ (风险环节)

乱序的风险几乎全部来自于 Canal 之后的**分布式处理环节**。

#### 风险点一：消息队列（MQ）的分区（Partitioning）

这是最常见的乱序原因。假设你使用 Kafka 作为中间的 MQ。

-   **场景**：Canal 默认的投递策略可能是按 `database` 或 `table` 名作为 key 来决定消息进入哪个 Kafka partition。如果你的消费端有多个实例（或多个线程），它们会分别消费不同的 partition。
    
    -   **问题**：如果对 `user` 表 `id=1` 的记录，先后进行了两次更新（`update1` 和 `update2`），这两条消息都会被发送到同一个 partition，因此能保证 `update1` 在 `update2` 之前被消费。**这是安全的**。
        
    -   **但是**，如果 Canal 的分区策略设置不当（例如随机分区），或者你手动修改了分区逻辑，`update1` 和 `update2` 就可能被发到**不同的 partition**。由于消费端的处理速度不同，完全可能消费 partition-2 的实例先处理完 `update2`，而消费 partition-1 的实例后处理 `update1`。**这就造成了数据覆盖和不一致**。
        
-   **解决方案**：
    
    -   **按主键分区**：确保**同一条记录**的所有变更操作（INSERT, UPDATE, DELETE）都进入到同一个 partition。通常，我们会使用数据行的**主键 ID** 作为消息的 key（例如 `hash(primary_key) % num_partitions`），这样 Kafka 就会把所有关于 `id=1` 的消息都路由到同一个 partition，从而保证了单行数据的处理顺序。Canal 的客户端配置是支持这种自定义分区策略的。
        

#### 风险点二：消费端的并发处理

即使你保证了同一主键的数据进入了同一个 partition，在消费端仍然可能出现乱序。

-   **场景**：你的消费程序为了提高吞吐量，启动了一个**多线程的线程池**来并发处理从 Kafka 拉取到的消息。
    
    1.  消费线程从一个 partition 中按顺序拉取到了 `update1` 和 `update2`。
        
    2.  线程 A 被分配去处理 `update1`，线程 B 被分配去处理 `update2`。
        
    3.  线程 A 因为某些原因（例如 GC a pause、网络抖动、ES集群负载高）处理得比较慢。
        
    4.  线程 B 处理得很快，先把 `update2` 的结果写入了 ES。
        
    5.  之后，线程 A 才处理完成，将 `update1` 的结果写入了 ES。
        
-   **结果**：ES 中该条记录的最终状态是 `update1` 的结果，而不是最新的 `update2`，**数据不一致**。
    
-   **解决方案**：
    
    1.  **单线程消费**：最简单的办法，让每个 partition 只由一个线程来处理。拉取消息和写入 ES 都在同一个线程里串行执行。缺点是吞吐量会受限。
        
    2.  **内存队列/Actor模型**：在消费端内部，可以设置一个分发逻辑。一个主线程负责从 Kafka 拉取数据，然后根据消息的主键 ID，将属于同一个 ID 的消息放入一个专属的内存队列（`ConcurrentHashMap<PrimaryKey, Queue<Message>>`）。然后工作线程池里的线程每次只从一个内存队列里取一个消息来处理。这样既保证了**单行数据的处理顺序性**，又实现了**不同行数据之间的并行处理**，兼顾了顺序和性能。
        

#### 风险点三：失败重试机制

如果消费端写入 ES 失败，进行重试，也可能导致乱序。例如，`update2` 写入成功了，但 `update1` 第一次写入失败，重试后成功了，也会导致数据被旧版本覆盖。

----------

### 3. Elasticsearch 的终极保障：版本控制

上述方案都是在“尽力而为”地保证处理顺序，但最好的做法是在最终的存储层也加上一道保险。Elasticsearch 提供了**乐观锁版本控制**机制，可以完美解决这个问题。

-   **使用外部版本号（External Versioning）**： ES 的 `index` API 允许你提供一个版本号。通过设置 `version_type=external`，ES 会遵循以下规则：
    
    > 只有当请求中的版本号**大于等于**文档当前存储的版本号时，操作才会成功。
    
-   **如何应用到 Canal 同步中**：
    
    1.  在 Canal 解析出的 binlog 事件中，一定有一个能代表事件顺序的值。最佳选择是 **binlog 事件的时间戳（executeTime）**。这个时间戳是 MySQL 生成的，能精确到毫秒，并且是严格递增的。
        
    2.  在你的消费端程序中，将数据写入 ES 时，带上版本号参数，并将版本号设置为这个 binlog 的时间戳。
        
        JSON
        
        ```
        // 伪代码，写入ES
        POST /my_index/_doc/1?version=1663999955000&version_type=external
        {
          "name": "new_value"
        }
        ```
        
    3.  **效果**：
        
        -   `update2` (时间戳 `t2`) 先到达，写入成功。当前文档版本为 `t2`。
            
        -   `update1` (时间戳 `t1`，且 `t1 < t2`) 后到达，尝试写入。ES 发现请求的版本 `t1` 小于当前存储的版本 `t2`，**会拒绝这次写入操作**，并返回一个版本冲突（version conflict）的错误。
            
        -   这样，无论消息处理顺序如何，ES 中存储的**永远是时间戳最新的那份数据**，从而保证了最终一致性。


# 幂等性
在分布式系统中，网络波动或服务超时等原因可能导致客户端重复提交请求。如果系统不做处理，就可能出现用户一次抽奖请求被执行多次，从而导致“多扣库存”、“多发奖品”等问题。幂等性（Idempotency）就是指对同一个接口的多次调用，所产生的结果和调用一次是相同的。

#### 方式一：针对外部业务号的幂等（如积分兑换、签到返利）

当用户通过某些行为（如使用积分兑换、每日签到）获取抽奖次数时，会调用 `createOrder` 方法。这个流程中会传入一个外部业务号 `outBusinessNo`。

-   **代码实现**： 在 `ActivityRepository.java` 的 `doSaveCreditPayOrder` 和 `doSaveNoPayOrder` 方法中，会将 `outBusinessNo` 连同订单信息一起存入 `raffle_activity_order` 表。这张表在数据库层面为 `out_business_no` 设置了唯一索引（`uq_out_business_no`）。
    
    -   如果因为网络重试等原因，使用同一个 `outBusinessNo` 重复请求创建订单，数据库在插入时会因为唯一索引冲突而抛出 `DuplicateKeyException` 异常。
        
    -   系统在业务代码中捕获了这个异常，并进行了事务回滚，从而保证了同一个业务请求不会被重复执行，避免了重复为用户增加抽奖次数。
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE0NzAwOTA4OTksLTk5NzgzOTcxNiwyMD
E5Njc1OTM4LDExMzQ0MTcxMDgsMTAyOTI4NTQzLC0xNDI0NTIw
MTcxLC0yMDczMjA1ODQ2LDE2NTUwNzY2MjMsMjAyNTY4OTQxNi
wtNzk1NTQwMDAsLTM0Nzg1MDgzLDE5NjY3MDQ0MjgsNTA1MzA5
ODQsLTEyOTU1MjUxNDcsMTU2OTExMzc2MV19
-->