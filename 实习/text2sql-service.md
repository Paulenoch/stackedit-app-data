# 直接字段选择策略 (The Fast Path)
-   **触发条件**:
    
    -   此策略的触发条件非常明确：当上一步的“表召回”过程只确定了一个相关的数据表时，系统会优先尝试使用直接字段选择策略。
        
    -   这个设计基于一个假设：如果只关联到一张表，那么用户的查询意图很可能相对直接，不涉及复杂的多表连接。
        
-   **工作机制**:
    
    1.  **不调用LLM**: 整个过程完全在服务内部通过代码逻辑完成，避免了调用大模型产生的网络延迟和API成本。
        
    2.  **提取查询实体**: 系统会首先从用户的自然语言问题中提取出实体词或关键词。
        
    3.  **规则化匹配**: 接着，系统会将这些实体词与那张唯一候选表的元数据进行匹配。匹配的目标包括：
        
        -   字段的原始名称 (Field Name)。
            
        -   字段的业务名称或别名 (Business Name)。
            
        -   字段的注释或描述 (Comment)。
            
        -   字段对应的维值。例如，如果用户问“查询‘男’性的数量”，系统能识别出“男”是“性别”字段的一个维值，从而选择“性别”字段。
            
    4.  **默认选择**: 为了保证查询的完整性，该表的所有主键字段会被默认选中。
        
-   **性能表现**:
    
    -   由于这个过程只涉及本地的计算和字符串匹配，所以执行速度极快，将字段选择的耗时从原本LLM所需的1-2秒缩短到了约0.1秒。
        

### 2. LLM 智能分析策略 (The Powerful Path)

当快速路径无法满足需求时，系统会无缝切换到基于大模型的智能分析策略。

-   **触发条件**:
    
    -   当“表召回”的结果大于一张表时，意味着查询可能需要进行 `JOIN` 操作，逻辑较为复杂。
        
    -   或者，当只召回了一张表，但“直接字段选择”策略未能成功匹配到任何有效字段时（除了默认的主键），系统也会认为查询意图比较模糊，需要更强的语义理解能力。
        
-   **工作机制**:
    
    -   在这种情况下，系统会将用户的自然语言问题、所有候选表的结构信息以及相关的知识库内容，一同打包发送给大语言模型 (LLM)。
        
    -   LLM会利用其强大的自然语言理解和推理能力，分析问题与众多字段之间的深层语义关系，最终给出一个或多个最相关的字段列表。
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTIxNDA4NDYxNV19
-->